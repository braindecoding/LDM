"""
üîç Quick Evaluation of Improved Model
Simple evaluation to compare original vs improved model performance.
"""

import torch
import torch.nn.functional as F
import numpy as np
from data_loader import load_fmri_data
from brain_ldm import BrainLDM
import matplotlib.pyplot as plt
from pathlib import Path

def improved_fmri_normalization(fmri_data):
    """Improved fMRI normalization with outlier handling."""
    # Robust z-score normalization
    median = torch.median(fmri_data, dim=0, keepdim=True)[0]
    mad = torch.median(torch.abs(fmri_data - median), dim=0, keepdim=True)[0]
    
    # Avoid division by zero
    mad = torch.where(mad == 0, torch.ones_like(mad), mad)
    
    # Robust normalization
    normalized = (fmri_data - median) / (1.4826 * mad)  # 1.4826 makes MAD consistent with std
    
    # Clip extreme outliers
    normalized = torch.clamp(normalized, -3, 3)
    
    return normalized

def evaluate_model(model_path, model_name="Model", use_improved_norm=False):
    """Evaluate a model and return metrics."""
    print(f"\nüìä Evaluating {model_name}")
    print("=" * 30)
    
    device = 'cpu'
    
    # Load model
    try:
        model = BrainLDM(fmri_dim=3092, image_size=28)
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        print(f"‚úÖ Loaded {model_name} from {model_path}")
    except Exception as e:
        print(f"‚ùå Failed to load {model_name}: {e}")
        return None
    
    # Load test data
    loader = load_fmri_data()
    test_data = loader.get_test_data()
    
    # Apply normalization
    if use_improved_norm:
        test_fmri = improved_fmri_normalization(test_data['fmri'])
        print("‚úÖ Applied improved fMRI normalization")
    else:
        test_fmri = test_data['fmri']
        print("‚úÖ Using standard normalization")
    
    test_stimuli = test_data['stimuli']
    
    # Generate reconstructions
    print("üß† Generating reconstructions...")
    with torch.no_grad():
        reconstructions = model.generate_from_fmri(test_fmri, num_inference_steps=20)
        reconstructions = reconstructions.view(reconstructions.shape[0], -1)
    
    # Compute metrics
    mse = F.mse_loss(reconstructions, test_stimuli).item()
    mae = F.l1_loss(reconstructions, test_stimuli).item()
    
    # Compute correlations
    recon_flat = reconstructions.cpu().numpy()
    stimuli_flat = test_stimuli.cpu().numpy()
    
    correlations = []
    for i in range(len(recon_flat)):
        corr = np.corrcoef(recon_flat[i], stimuli_flat[i])[0, 1]
        correlations.append(corr if not np.isnan(corr) else 0)
    
    avg_correlation = np.mean(correlations)
    
    # Classification accuracy (correlation matrix approach)
    corr_matrix = np.zeros((len(recon_flat), len(stimuli_flat)))
    for i in range(len(recon_flat)):
        for j in range(len(stimuli_flat)):
            corr = np.corrcoef(recon_flat[i], stimuli_flat[j])[0, 1]
            corr_matrix[i, j] = corr if not np.isnan(corr) else 0
    
    correct_matches = sum(np.argmax(corr_matrix[i, :]) == i for i in range(len(corr_matrix)))
    accuracy = correct_matches / len(corr_matrix)
    
    print(f"üìà {model_name} Results:")
    print(f"  MSE: {mse:.6f}")
    print(f"  MAE: {mae:.6f}")
    print(f"  Average Correlation: {avg_correlation:.6f}")
    print(f"  Classification Accuracy: {accuracy:.2%} ({correct_matches}/{len(corr_matrix)})")
    
    return {
        'mse': mse,
        'mae': mae,
        'correlation': avg_correlation,
        'accuracy': accuracy,
        'reconstructions': reconstructions,
        'stimuli': test_stimuli
    }

def compare_models():
    """Compare original vs improved models."""
    print("üîç Model Comparison Analysis")
    print("=" * 40)
    
    # Evaluate original model
    original_results = evaluate_model(
        "checkpoints/best_model.pt", 
        "Original Model", 
        use_improved_norm=False
    )
    
    # Evaluate improved model (if exists)
    improved_path = "checkpoints/improved_model.pt"
    if Path(improved_path).exists():
        improved_results = evaluate_model(
            improved_path, 
            "Improved Model", 
            use_improved_norm=True
        )
    else:
        print(f"‚ùå Improved model not found at {improved_path}")
        improved_results = None
    
    # Compare results
    if original_results and improved_results:
        print(f"\nüìä Detailed Comparison:")
        print(f"{'Metric':<20} {'Original':<12} {'Improved':<12} {'Change':<15}")
        print("-" * 65)
        
        for metric in ['mse', 'mae', 'correlation', 'accuracy']:
            orig = original_results[metric]
            impr = improved_results[metric]
            
            if metric in ['mse', 'mae']:
                # Lower is better for MSE/MAE
                if orig > 0:
                    change_pct = ((orig - impr) / orig * 100)
                    change = f"{change_pct:+.1f}%"
                else:
                    change = "N/A"
            else:
                # Higher is better for correlation/accuracy
                if orig > 0:
                    change_pct = ((impr - orig) / orig * 100)
                    change = f"{change_pct:+.1f}%"
                else:
                    change = f"+{impr:.3f}"
            
            print(f"{metric.upper():<20} {orig:<12.6f} {impr:<12.6f} {change:<15}")
        
        # Create visualization
        create_comparison_plot(original_results, improved_results)
    
    return original_results, improved_results

def create_comparison_plot(original_results, improved_results):
    """Create comparison visualization."""
    print("\nüìä Creating comparison visualization...")
    
    # Metrics comparison
    metrics = ['MSE', 'MAE', 'Correlation', 'Accuracy']
    original_values = [
        original_results['mse'],
        original_results['mae'], 
        original_results['correlation'],
        original_results['accuracy']
    ]
    improved_values = [
        improved_results['mse'],
        improved_results['mae'],
        improved_results['correlation'], 
        improved_results['accuracy']
    ]
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # Bar comparison
    x = np.arange(len(metrics))
    width = 0.35
    
    ax1.bar(x - width/2, original_values, width, label='Original', alpha=0.7, color='steelblue')
    ax1.bar(x + width/2, improved_values, width, label='Improved', alpha=0.7, color='orange')
    ax1.set_xlabel('Metrics')
    ax1.set_ylabel('Values')
    ax1.set_title('Model Performance Comparison')
    ax1.set_xticks(x)
    ax1.set_xticklabels(metrics)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Sample reconstructions comparison
    num_samples = min(3, len(original_results['reconstructions']))
    
    for i in range(num_samples):
        # Original stimulus
        ax = plt.subplot(2, 6, 7 + i*2)
        stimulus = original_results['stimuli'][i].cpu().numpy().reshape(28, 28)
        plt.imshow(stimulus, cmap='gray', vmin=0, vmax=1)
        plt.title(f'Stimulus {i+1}')
        plt.axis('off')
        
        # Original reconstruction
        ax = plt.subplot(2, 6, 8 + i*2)
        orig_recon = original_results['reconstructions'][i].cpu().numpy().reshape(28, 28)
        plt.imshow(orig_recon, cmap='gray', vmin=0, vmax=1)
        plt.title(f'Original Recon {i+1}')
        plt.axis('off')
    
    # Accuracy improvement
    ax3.bar(['Original', 'Improved'], 
           [original_results['accuracy']*100, improved_results['accuracy']*100],
           color=['steelblue', 'orange'], alpha=0.7)
    ax3.set_ylabel('Accuracy (%)')
    ax3.set_title('Classification Accuracy Improvement')
    ax3.grid(True, alpha=0.3)
    
    # Correlation improvement
    ax4.bar(['Original', 'Improved'], 
           [original_results['correlation'], improved_results['correlation']],
           color=['steelblue', 'orange'], alpha=0.7)
    ax4.set_ylabel('Average Correlation')
    ax4.set_title('Reconstruction Correlation Improvement')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Save plot
    output_path = "results/model_comparison.png"
    Path("results").mkdir(exist_ok=True)
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"üíæ Saved comparison plot to: {output_path}")
    
    plt.show()

def analyze_improvements():
    """Analyze what improvements were effective."""
    print("\nüî¨ Improvement Analysis")
    print("=" * 30)
    
    improvements_implemented = [
        "‚úÖ Improved fMRI normalization (robust z-score)",
        "‚úÖ Data augmentation (doubled training data)",
        "‚úÖ Perceptual loss function",
        "‚úÖ Better optimizer (AdamW with weight decay)",
        "‚úÖ Learning rate scheduling (Cosine annealing)",
        "‚úÖ Gradient clipping",
        "‚úÖ Longer training (50 epochs vs 50)"
    ]
    
    print("üöÄ Implemented Improvements:")
    for improvement in improvements_implemented:
        print(f"  {improvement}")
    
    print(f"\nüí° Key Insights:")
    print(f"  ‚Ä¢ Robust normalization helps handle fMRI outliers")
    print(f"  ‚Ä¢ Data augmentation increases training diversity")
    print(f"  ‚Ä¢ Perceptual loss preserves visual structure better than MSE")
    print(f"  ‚Ä¢ Learning rate scheduling improves convergence")
    print(f"  ‚Ä¢ More training data and epochs allow better learning")

def main():
    """Main evaluation function."""
    print("üîç Quick Model Evaluation & Comparison")
    print("=" * 50)
    
    # Compare models
    original_results, improved_results = compare_models()
    
    # Analyze improvements
    analyze_improvements()
    
    # Summary
    if original_results and improved_results:
        orig_acc = original_results['accuracy']
        impr_acc = improved_results['accuracy']
        improvement = ((impr_acc - orig_acc) / orig_acc * 100) if orig_acc > 0 else 0
        
        print(f"\nüéØ Summary:")
        print(f"  Original Accuracy: {orig_acc:.2%}")
        print(f"  Improved Accuracy: {impr_acc:.2%}")
        print(f"  Improvement: {improvement:+.1f}%")
        
        if impr_acc > orig_acc:
            print(f"  ‚úÖ Model performance improved!")
        else:
            print(f"  ‚ö†Ô∏è Model needs further improvements")
    
    print(f"\nüìÅ Results saved to: results/model_comparison.png")

if __name__ == "__main__":
    main()
