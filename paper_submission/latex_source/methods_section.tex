\section{Methods}

\subsection{Dataset and Preprocessing}

We utilized the publicly available fMRI-digit dataset~\cite{dataset_ref}, consisting of functional magnetic resonance imaging (fMRI) responses from visual cortex during digit perception tasks. The dataset comprises 120 samples total: 90 training samples and 30 test samples, with each sample containing fMRI signals from 3,092 voxels and corresponding 28×28 pixel grayscale digit images (digits 0-9).

\subsubsection{fMRI Signal Preprocessing}
fMRI signals underwent robust normalization using median absolute deviation (MAD) to handle outliers:
\begin{equation}
\mathbf{x}_{\text{norm}} = \frac{\mathbf{x} - \text{median}(\mathbf{x})}{1.4826 \cdot \text{MAD}(\mathbf{x})}
\end{equation}
where $\mathbf{x} \in \mathbb{R}^{3092}$ represents the raw fMRI signal, and normalized signals were clipped to $[-3, 3]$ to ensure stability.

\subsubsection{Data Augmentation}
To address the limited dataset size, we implemented a comprehensive 10× augmentation strategy:
\begin{itemize}
    \item \textbf{Progressive noise injection}: Gaussian noise with levels $\sigma \in [0.01, 0.19]$
    \item \textbf{Feature dropout}: Random masking of 2-11\% of fMRI features
    \item \textbf{Signal scaling}: Multiplicative factors sampled from $\mathcal{U}(0.9, 1.1)$
    \item \textbf{Smooth perturbations}: Low-amplitude Gaussian noise ($\sigma = 0.005$)
\end{itemize}

\subsection{Multi-Modal Brain Latent Diffusion Model}

\subsubsection{Architecture Overview}
Our proposed model integrates three modalities through a unified latent diffusion framework:

\begin{equation}
\mathbf{y} = f_{\theta}(\mathbf{x}_{\text{fMRI}}, \mathbf{t}_{\text{text}}, \mathbf{s}_{\text{semantic}})
\end{equation}

where $\mathbf{x}_{\text{fMRI}} \in \mathbb{R}^{3092}$ is the fMRI signal, $\mathbf{t}_{\text{text}}$ represents text embeddings, $\mathbf{s}_{\text{semantic}}$ denotes semantic class embeddings, and $\mathbf{y} \in \mathbb{R}^{28 \times 28}$ is the reconstructed image.

\subsubsection{fMRI Encoder}
The fMRI encoder transforms neural signals into a latent representation:
\begin{align}
\mathbf{h}_1 &= \text{ReLU}(\text{LayerNorm}(\mathbf{W}_1 \mathbf{x}_{\text{fMRI}} + \mathbf{b}_1)) \\
\mathbf{h}_2 &= \text{Dropout}(\mathbf{h}_1, p=0.3) \\
\mathbf{z}_{\text{fMRI}} &= \text{ReLU}(\text{LayerNorm}(\mathbf{W}_2 \mathbf{h}_2 + \mathbf{b}_2))
\end{align}
where $\mathbf{W}_1 \in \mathbb{R}^{1024 \times 3092}$, $\mathbf{W}_2 \in \mathbb{R}^{512 \times 1024}$, and $\mathbf{z}_{\text{fMRI}} \in \mathbb{R}^{512}$.

\subsubsection{Text Encoder}
Text guidance utilizes a transformer-based encoder with 4 layers:
\begin{equation}
\mathbf{z}_{\text{text}} = \text{Transformer}(\text{Embedding}(\mathbf{t}_{\text{text}}))
\end{equation}
Text prompts follow templates: "A handwritten digit [digit\_name]", "The number [digit\_name]", etc.

\subsubsection{Cross-Modal Attention}
Multi-modal features are fused through cross-modal attention:
\begin{align}
\mathbf{Q} &= \mathbf{z}_{\text{fMRI}} \mathbf{W}_Q, \quad \mathbf{K} = [\mathbf{z}_{\text{text}}; \mathbf{z}_{\text{semantic}}] \mathbf{W}_K \\
\mathbf{V} &= [\mathbf{z}_{\text{text}}; \mathbf{z}_{\text{semantic}}] \mathbf{W}_V \\
\mathbf{z}_{\text{fused}} &= \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}
\end{align}

\subsubsection{Conditional U-Net}
The diffusion process employs a U-Net architecture with skip connections and condition injection:
\begin{equation}
\mathbf{y}_t = \text{U-Net}(\mathbf{y}_{t+1}, t, \mathbf{z}_{\text{fused}})
\end{equation}
where $t$ represents the diffusion timestep and $\mathbf{z}_{\text{fused}}$ provides conditional guidance.

\subsection{Uncertainty Quantification}

\subsubsection{Monte Carlo Dropout}
We implement Monte Carlo dropout sampling to estimate epistemic uncertainty:
\begin{equation}
\mathbf{y}_i = f_{\theta}(\mathbf{x}_{\text{fMRI}} + \boldsymbol{\epsilon}_i, \text{dropout}=\text{True}), \quad i = 1, \ldots, N
\end{equation}
where $\boldsymbol{\epsilon}_i \sim \mathcal{N}(0, 0.05^2 \mathbf{I})$ and $N = 30$ samples.

\subsubsection{Uncertainty Estimation}
Epistemic and aleatoric uncertainties are computed as:
\begin{align}
\sigma_{\text{epistemic}}^2 &= \frac{1}{N} \sum_{i=1}^N (\mathbf{y}_i - \bar{\mathbf{y}})^2 \\
\sigma_{\text{aleatoric}}^2 &= \frac{1}{N} \sum_{i=1}^N \sigma_i^2(\mathbf{x})
\end{align}
where $\bar{\mathbf{y}} = \frac{1}{N} \sum_{i=1}^N \mathbf{y}_i$ and $\sigma_i^2(\mathbf{x})$ is the predicted aleatoric variance.

\subsubsection{Temperature Scaling}
For calibration, we employ learnable temperature scaling:
\begin{equation}
p_{\text{calibrated}} = \text{softmax}\left(\frac{\mathbf{z}}{T}\right)
\end{equation}
where $T$ is a learnable parameter initialized to 1.0 and optimized during training.

\subsection{Training Procedure}

\subsubsection{Loss Function}
The total loss combines reconstruction, perceptual, and uncertainty components:
\begin{align}
\mathcal{L}_{\text{total}} &= \mathcal{L}_{\text{recon}} + \lambda_p \mathcal{L}_{\text{perceptual}} + \lambda_u \mathcal{L}_{\text{uncertainty}} \\
\mathcal{L}_{\text{recon}} &= \|\mathbf{y} - \mathbf{y}_{\text{target}}\|_2^2 \\
\mathcal{L}_{\text{perceptual}} &= \|\nabla \mathbf{y} - \nabla \mathbf{y}_{\text{target}}\|_2^2 \\
\mathcal{L}_{\text{uncertainty}} &= \|\sigma_{\text{pred}}^2 - \sigma_{\text{target}}^2\|_2^2
\end{align}
with dynamic weighting: $\lambda_p = 0.1(1 + \frac{\text{epoch}}{\text{total\_epochs}})$, $\lambda_u = 0.01(1 + 2\frac{\text{epoch}}{\text{total\_epochs}})$.

\subsubsection{Optimization}
We employ component-specific learning rates with AdamW optimizer:
\begin{itemize}
    \item fMRI encoder: $8 \times 10^{-5}$
    \item Text encoder: $4 \times 10^{-5}$
    \item Cross-modal attention: $1.2 \times 10^{-4}$
    \item U-Net: $8 \times 10^{-5}$
    \item Temperature parameter: $8 \times 10^{-6}$
\end{itemize}
Weight decay was set to $5 \times 10^{-6}$ with gradient clipping at norm 1.0.

\subsubsection{Learning Rate Scheduling}
Cosine annealing with warm restarts was applied:
\begin{equation}
\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})(1 + \cos(\frac{T_{\text{cur}}}{T_i}\pi))
\end{equation}
with $T_0 = 20$, $T_{\text{mult}} = 2$, and $\eta_{\min} = 10^{-7}$.

\subsection{Evaluation Metrics}

\subsubsection{Reconstruction Quality}
\begin{itemize}
    \item \textbf{Classification Accuracy}: Percentage of correctly identified digits via correlation matrix
    \item \textbf{Pixel Correlation}: Pearson correlation between reconstructed and target images
    \item \textbf{Mean Squared Error}: $\text{MSE} = \frac{1}{HW}\|\mathbf{y} - \mathbf{y}_{\text{target}}\|_2^2$
\end{itemize}

\subsubsection{Uncertainty Calibration}
\begin{itemize}
    \item \textbf{Uncertainty-Error Correlation}: Pearson correlation between prediction uncertainty and reconstruction error
    \item \textbf{Calibration Ratio}: Ratio of low-uncertainty to high-uncertainty errors
    \item \textbf{Expected Calibration Error}: $\text{ECE} = \sum_{m=1}^M \frac{|B_m|}{n}|\text{acc}(B_m) - \text{conf}(B_m)|$
\end{itemize}

\subsection{Implementation Details}

All experiments were conducted using PyTorch 2.0 on CPU with 16GB RAM. Training employed batch size 4 for 150 epochs with early stopping (patience=25). Random seeds were fixed (seed=42) for reproducibility. The final model contains 58.2M parameters and achieved convergence after 140 epochs with 98.7\% loss reduction.

\subsection{Statistical Analysis}

Statistical significance was assessed using paired t-tests for performance comparisons between models. Confidence intervals were computed using bootstrap resampling with 1000 iterations. Multiple comparison corrections were applied using the Benjamini-Hochberg procedure with false discovery rate $\alpha = 0.05$.

\subsubsection{Cross-Validation}
Due to the limited dataset size, we employed stratified 5-fold cross-validation to ensure robust performance estimates. Each fold maintained balanced digit representation across training and validation sets.

\subsubsection{Baseline Comparisons}
We compared our approach against three baseline methods:
\begin{itemize}
    \item \textbf{Linear Regression}: Direct fMRI-to-image mapping
    \item \textbf{Standard VAE}: Variational autoencoder without uncertainty quantification
    \item \textbf{Basic LDM}: Latent diffusion model without multi-modal guidance
\end{itemize}

\subsection{Reproducibility and Code Availability}

All code, trained models, and experimental configurations are publicly available at \url{https://github.com/[username]/Brain-LDM-Uncertainty}. The repository includes:
\begin{itemize}
    \item Complete source code with documentation
    \item Pre-trained model weights (best\_improved\_v1\_model.pt)
    \item Evaluation scripts and metrics computation
    \item Visualization tools for uncertainty analysis
    \item Docker container for environment reproducibility
\end{itemize}

Computational requirements: 16GB RAM, 4-core CPU, approximately 3.2 hours training time. No GPU required for inference or training.
